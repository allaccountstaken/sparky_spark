{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SpamDetector').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.option('header', 'true').csv('spam.csv', inferSchema=True)\n",
    "sms = data.select(data.columns[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- v1: string (nullable = true)\n",
      " |-- v2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|  v1|                  v2|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "+----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms = sms.withColumnRenamed(\"v1\",\"label\").withColumnRenamed(\"v2\",\"text\")\n",
    "sms.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                text|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|[go, until, juron...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "wrangled.show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled = StopWordsRemover(inputCol='words', outputCol='terms').transform(wrangled)\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled = HashingTF(inputCol='terms', outputCol='hash', numFeatures=1024).transform(wrangled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|                text|               words|               terms|                hash|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|(1024,[12,171,191...|\n",
      "|  ham|Ok lar Joking wif...|[ok, lar, joking,...|[ok, lar, joking,...|(1024,[3,493,565,...|\n",
      "| spam|Free entry in a w...|[free, entry, in,...|[free, entry, wkl...|(1024,[16,24,35,5...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wrangled.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wrangled.hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o468.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 31) (172.20.10.30 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$2941/0x00000008018a1ba8: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: java.lang.NullPointerException: Cannot invoke \"String.toLowerCase()\" because \"x$1\" is null\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 30 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:55)\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:93)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$2941/0x00000008018a1ba8: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b9dab825204b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert hashed symbols to TF-IDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrangled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrangled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#tf_idf.select('terms', 'features').show(4, truncate=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o468.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 31) (172.20.10.30 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$2941/0x00000008018a1ba8: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: java.lang.NullPointerException: Cannot invoke \"String.toLowerCase()\" because \"x$1\" is null\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 30 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:55)\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:93)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (Tokenizer$$Lambda$2941/0x00000008018a1ba8: (string) => array<string>)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.Tokenizer.$anonfun$createTransformFunc$1(Tokenizer.scala:40)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol='hash', outputCol='features').fit(wrangled).transform(wrangled)\n",
    "      \n",
    "#tf_idf.select('terms', 'features').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "sms_train, sms_test = sms.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "# Fit a Logistic Regression model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "prediction = logistic.transform(sms_test)                      \n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Accuracy=(TN+TP)/(TN+TP+FN+FP)â€”proportionofcorrectpredictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Break text into tokens at non-word characters\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='terms')\n",
    "\n",
    "# Apply the hashing trick and transform to TF-IDF\n",
    "hasher = HashingTF(inputCol=\"terms\", outputCol=\"hash\")\n",
    "idf = IDF(inputCol=\"hash\", outputCol=\"features\")\n",
    "\n",
    "# Create a logistic regression object and add everything to a pipeline\n",
    "logistic = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regex and NLTK examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing list of Tweets:\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "hashtags = regexp_tokenize(tweets[2], pattern1)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "source": [
    "#Finding `mentions` symbol in the tweets \n",
    "#source: https://stackoverflow.com/questions/7150652/regex-valid-twitter-mention\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# Using list comprehension to tokenize the entire list of tweets, \n",
    "# instanciate an object for this:\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing:\n",
    "lower_tokens=[\"'\",\n",
    " \"''\",\n",
    " 'debugging',\n",
    " \"''\",\n",
    " \"'\",\n",
    " 'is',\n",
    " 'the',\n",
    " 'process',\n",
    " 'of',\n",
    " 'finding',\n",
    " 'and',\n",
    " 'resolving',\n",
    " 'of',\n",
    " 'defects',\n",
    " 'that',\n",
    " 'prevent',\n",
    " 'correct',\n",
    " 'operation',\n",
    " 'of',\n",
    " 'computer',\n",
    " 'software',\n",
    " 'or',\n",
    " 'a',\n",
    " 'system',\n",
    " '.',\n",
    " 'numerous',\n",
    " 'books',\n",
    " 'have',\n",
    " 'been',\n",
    " 'written',\n",
    " 'about',\n",
    " 'debugging',\n",
    " '(',\n",
    " 'see',\n",
    " 'below',\n",
    " ':',\n",
    " '#',\n",
    " 'further',\n",
    " 'reading|further',\n",
    " 'reading',\n",
    " ')',\n",
    " ',',\n",
    " 'as',\n",
    " 'it',\n",
    " 'involves',\n",
    " 'numerous',\n",
    " 'aspects',\n",
    " ',',\n",
    " 'including',\n",
    " 'interactive',\n",
    " 'debugging',\n",
    " ',',\n",
    " 'control',\n",
    " 'flow',\n",
    " ',',\n",
    " 'integration',\n",
    " 'testing',\n",
    " ',',\n",
    " 'logfile|log',\n",
    " 'files',\n",
    " ',',\n",
    " 'monitoring',\n",
    " '(',\n",
    " 'application',\n",
    " 'monitoring|application',\n",
    " ',',\n",
    " 'system',\n",
    " 'monitoring|system',\n",
    " ')',\n",
    " ',',\n",
    " 'memory',\n",
    " 'dumps',\n",
    " ',',\n",
    " 'profiling',\n",
    " '(',\n",
    " 'computer',\n",
    " 'programming',\n",
    " ')',\n",
    " '|profiling',\n",
    " ',',\n",
    " 'statistical',\n",
    " 'process',\n",
    " 'control',\n",
    " ',',\n",
    " 'and',\n",
    " 'special',\n",
    " 'design',\n",
    " 'tactics',\n",
    " 'to',\n",
    " 'improve',\n",
    " 'detection',\n",
    " 'while',\n",
    " 'simplifying',\n",
    " 'changes',\n",
    " '.',\n",
    " 'origin',\n",
    " 'a',\n",
    " 'computer',\n",
    " 'log',\n",
    " 'entry',\n",
    " 'from',\n",
    " 'the',\n",
    " 'mark',\n",
    " '&',\n",
    " 'nbsp',\n",
    " ';',\n",
    " 'ii',\n",
    " ',',\n",
    " 'with',\n",
    " 'a',\n",
    " 'moth',\n",
    " 'taped',\n",
    " 'to',\n",
    " 'the',\n",
    " 'page',\n",
    " 'the',\n",
    " 'terms',\n",
    " '``',\n",
    " 'bug',\n",
    " \"''\",\n",
    " 'and',\n",
    " '``',\n",
    " 'debugging',\n",
    " \"''\",\n",
    " 'are',\n",
    " 'popularly',\n",
    " 'attributed',\n",
    " 'to',\n",
    " 'admiral',\n",
    " 'grace',\n",
    " 'hopper',\n",
    " 'in',\n",
    " 'the',\n",
    " '1940s',\n",
    " '.',\n",
    " '[',\n",
    " 'http',\n",
    " ':',\n",
    " '//foldoc.org/grace+hopper',\n",
    " 'grace',\n",
    " 'hopper',\n",
    " ']',\n",
    " 'from',\n",
    " 'foldoc',\n",
    " 'while',\n",
    " 'she',\n",
    " 'was',\n",
    " 'working',\n",
    " 'on',\n",
    " 'a',\n",
    " 'harvard',\n",
    " 'mark',\n",
    " 'ii|mark',\n",
    " 'ii',\n",
    " 'computer',\n",
    " 'at',\n",
    " 'harvard',\n",
    " 'university',\n",
    " ',',\n",
    " 'her',\n",
    " 'associates',\n",
    " 'discovered',\n",
    " 'a',\n",
    " 'moth',\n",
    " 'stuck',\n",
    " 'in',\n",
    " 'a',\n",
    " 'relay',\n",
    " 'and',\n",
    " 'thereby',\n",
    " 'impeding',\n",
    " 'operation',\n",
    " ',',\n",
    " 'whereupon',\n",
    " 'she',\n",
    " 'remarked',\n",
    " 'that',\n",
    " 'they',\n",
    " 'were',\n",
    " '``',\n",
    " 'debugging',\n",
    " \"''\",\n",
    " 'the',\n",
    " 'system',\n",
    " '.',\n",
    " 'however',\n",
    " 'the',\n",
    " 'term',\n",
    " '``',\n",
    " 'bug',\n",
    " \"''\",\n",
    " 'in',\n",
    " 'the',\n",
    " 'meaning',\n",
    " 'of',\n",
    " 'technical',\n",
    " 'error',\n",
    " 'dates',\n",
    " 'back',\n",
    " 'at',\n",
    " 'least',\n",
    " 'to',\n",
    " '1878',\n",
    " 'and',\n",
    " 'thomas',\n",
    " 'edison',\n",
    " '(',\n",
    " 'see',\n",
    " 'software',\n",
    " 'bug',\n",
    " 'for',\n",
    " 'a',\n",
    " 'full',\n",
    " 'discussion',\n",
    " ')',\n",
    " ',',\n",
    " 'and',\n",
    " '``',\n",
    " 'debugging',\n",
    " \"''\",\n",
    " 'seems',\n",
    " 'to',\n",
    " 'have',\n",
    " 'been',\n",
    " 'used',\n",
    " 'as',\n",
    " 'a',\n",
    " 'term',\n",
    " 'in',\n",
    " 'aeronautics',\n",
    " 'before',\n",
    " 'entering',\n",
    " 'the',\n",
    " 'world',\n",
    " 'of',\n",
    " 'computers',\n",
    " '.',\n",
    " 'indeed',\n",
    " ',',\n",
    " 'in',\n",
    " 'an',\n",
    " 'interview',\n",
    " 'grace',\n",
    " 'hopper',\n",
    " 'remarked',\n",
    " 'that',\n",
    " 'she',\n",
    " 'was',\n",
    " 'not',\n",
    " 'coining',\n",
    " 'the',\n",
    " 'term',\n",
    " '{',\n",
    " '{',\n",
    " 'citation',\n",
    " 'needed|date=july',\n",
    " '2015',\n",
    " '}',\n",
    " '}',\n",
    " '.',\n",
    " 'the',\n",
    " 'moth',\n",
    " 'fit',\n",
    " 'the',\n",
    " 'already',\n",
    " 'existing',\n",
    " 'terminology',\n",
    " ',',\n",
    " 'so',\n",
    " 'it',\n",
    " 'was',\n",
    " 'saved',\n",
    " '.',\n",
    " 'a',\n",
    " 'letter',\n",
    " 'from',\n",
    " 'j.',\n",
    " 'robert',\n",
    " 'oppenheimer',\n",
    " '(',\n",
    " 'director',\n",
    " 'of',\n",
    " 'the',\n",
    " 'wwii',\n",
    " 'atomic',\n",
    " 'bomb',\n",
    " '``',\n",
    " 'manhattan',\n",
    " \"''\",\n",
    " 'project',\n",
    " 'at',\n",
    " 'los',\n",
    " 'alamos',\n",
    " ',',\n",
    " 'nm',\n",
    " ')',\n",
    " 'used',\n",
    " 'the',\n",
    " 'term',\n",
    " 'in',\n",
    " 'a',\n",
    " 'letter',\n",
    " 'to',\n",
    " 'dr.',\n",
    " 'ernest',\n",
    " 'lawrence',\n",
    " 'at',\n",
    " 'uc',\n",
    " 'berkeley',\n",
    " ',',\n",
    " 'dated',\n",
    " 'october',\n",
    " '27',\n",
    " ',',\n",
    " '1944',\n",
    " ',',\n",
    " 'http',\n",
    " ':',\n",
    " '//bancroft.berkeley.edu/exhibits/physics/images/bigscience25.jpg',\n",
    " 'regarding',\n",
    " 'the',\n",
    " 'recruitment',\n",
    " 'of',\n",
    " 'additional',\n",
    " 'technical',\n",
    " 'staff',\n",
    " '.',\n",
    " 'the',\n",
    " 'oxford',\n",
    " 'english',\n",
    " 'dictionary',\n",
    " 'entry',\n",
    " 'for',\n",
    " '``',\n",
    " 'debug',\n",
    " \"''\",\n",
    " 'quotes',\n",
    " 'the',\n",
    " 'term',\n",
    " '``',\n",
    " 'debugging',\n",
    " \"''\",\n",
    " 'used',\n",
    " 'in',\n",
    " 'reference',\n",
    " 'to',\n",
    " 'airplane',\n",
    " 'engine',\n",
    " 'testing',\n",
    " 'in',\n",
    " 'a',\n",
    " '1945',\n",
    " 'article',\n",
    " 'in',\n",
    " 'the',\n",
    " 'journal',\n",
    " 'of',\n",
    " 'the',\n",
    " 'royal',\n",
    " 'aeronautical',\n",
    " 'society',\n",
    " '.',\n",
    " 'an',\n",
    " 'article',\n",
    " 'in',\n",
    " '``',\n",
    " 'airforce',\n",
    " \"''\",\n",
    " '(',\n",
    " 'june',\n",
    " '1945',\n",
    " 'p.',\n",
    " '&',\n",
    " 'nbsp',\n",
    " ';',\n",
    " '50',\n",
    " ')',\n",
    " 'also',\n",
    " 'refers',\n",
    " 'to',\n",
    " 'debugging',\n",
    " ',',\n",
    " 'this',\n",
    " 'time',\n",
    " 'of',\n",
    " 'aircraft',\n",
    " 'cameras',\n",
    " '.',\n",
    " 'hopper',\n",
    " \"'s\",\n",
    " 'computer',\n",
    " 'bug|bug',\n",
    " 'was',\n",
    " 'found',\n",
    " 'on',\n",
    " 'september',\n",
    " '9',\n",
    " ',',\n",
    " '1947',\n",
    " '.',\n",
    " 'the',\n",
    " 'term',\n",
    " 'was',\n",
    " 'not',\n",
    " 'adopted',\n",
    " 'by',\n",
    " 'computer',\n",
    " 'programmers',\n",
    " 'until',\n",
    " 'the',\n",
    " 'early',\n",
    " '1950s',\n",
    " '.',\n",
    " 'the',\n",
    " 'seminal',\n",
    " 'article',\n",
    " 'by',\n",
    " 'gills',\n",
    " '.',\n",
    " 'gill',\n",
    " ',',\n",
    " '[',\n",
    " 'http',\n",
    " ':',\n",
    " '//www.jstor.org/stable/98663',\n",
    " 'the',\n",
    " 'diagnosis',\n",
    " 'of',\n",
    " 'mistakes',\n",
    " 'in',\n",
    " 'programmes',\n",
    " 'on',\n",
    " 'the',\n",
    " 'edsac',\n",
    " ']',\n",
    " ',',\n",
    " 'proceedings',\n",
    " 'of',\n",
    " 'the',\n",
    " 'royal',\n",
    " 'society',\n",
    " 'of',\n",
    " 'london',\n",
    " '.',\n",
    " 'series',\n",
    " 'a',\n",
    " ',',\n",
    " 'mathematical',\n",
    " 'and',\n",
    " 'physical',\n",
    " 'sciences',\n",
    " ',',\n",
    " 'vol',\n",
    " '.',\n",
    " '206',\n",
    " ',',\n",
    " 'no',\n",
    " '.',\n",
    " '1087',\n",
    " '(',\n",
    " 'may',\n",
    " '22',\n",
    " ',',\n",
    " '1951',\n",
    " ')',\n",
    " ',',\n",
    " 'pp',\n",
    " '.',\n",
    " '538-554',\n",
    " 'in',\n",
    " '1951',\n",
    " 'is',\n",
    " 'the',\n",
    " 'earliest',\n",
    " 'in-depth',\n",
    " 'discussion',\n",
    " 'of',\n",
    " 'programming',\n",
    " 'errors',\n",
    " ',',\n",
    " 'but',\n",
    " 'it',\n",
    " 'does',\n",
    " 'not',\n",
    " 'use',\n",
    " 'the',\n",
    " 'term',\n",
    " '``',\n",
    " 'bug',\n",
    " \"''\",\n",
    " 'or',\n",
    " '``',\n",
    " 'debugging',\n",
    " \"''\",\n",
    " '.',\n",
    " 'in',\n",
    " 'the',\n",
    " 'association',\n",
    " 'for',\n",
    " 'computing',\n",
    " 'machinery|acm',\n",
    " \"'s\",\n",
    " 'digital',\n",
    " 'library',\n",
    " ',',\n",
    " 'the',\n",
    " 'term',\n",
    " '``',\n",
    " 'debugging',\n",
    " \"''\",\n",
    " 'is',\n",
    " 'first',\n",
    " 'used',\n",
    " 'in',\n",
    " 'three',\n",
    " 'papers',\n",
    " 'from',\n",
    " '1952',\n",
    " 'acm',\n",
    " 'national',\n",
    " 'meetings.robert',\n",
    " 'v.',\n",
    " 'd.',\n",
    " 'campbell',\n",
    " ',',\n",
    " '[',\n",
    " 'http',\n",
    " ':',\n",
    " '//portal.acm.org/citation.cfm',\n",
    " '?',\n",
    " 'id=609784.609786',\n",
    " 'evolution',\n",
    " 'of',\n",
    " 'automatic',\n",
    " 'computation',\n",
    " ']',\n",
    " ',',\n",
    " 'proceedings',\n",
    " 'of',\n",
    " 'the',\n",
    " '1952',\n",
    " 'acm',\n",
    " 'national',\n",
    " 'meeting',\n",
    " '(',\n",
    " 'pittsburgh',\n",
    " ')',\n",
    " ',',\n",
    " 'p',\n",
    " '29-32',\n",
    " ',',\n",
    " '1952.alex',\n",
    " 'orden',\n",
    " ',',\n",
    " '[',\n",
    " 'http',\n",
    " ':',\n",
    " '//portal.acm.org/citation.cfm',\n",
    " '?',\n",
    " 'id=609784.609793',\n",
    " 'solution',\n",
    " 'of',\n",
    " 'systems',\n",
    " 'of',\n",
    " 'linear',\n",
    " 'inequalities',\n",
    " 'on',\n",
    " 'a',\n",
    " 'digital',\n",
    " 'computer',\n",
    " ']',\n",
    " ',',\n",
    " 'proceedings',\n",
    " 'of',\n",
    " 'the',\n",
    " '1952',\n",
    " 'acm',\n",
    " 'national',\n",
    " 'meeting',\n",
    " '(',\n",
    " 'pittsburgh',\n",
    " ')',\n",
    " ',',\n",
    " 'p.',\n",
    " '91-95',\n",
    " ',',\n",
    " '1952.howard',\n",
    " 'b.',\n",
    " 'demuth',\n",
    " ',',\n",
    " 'john',\n",
    " 'b.',\n",
    " 'jackson',\n",
    " ',',\n",
    " 'edmund',\n",
    " 'klein',\n",
    " ',',\n",
    " 'n.',\n",
    " 'metropolis',\n",
    " ',',\n",
    " 'walter',\n",
    " 'orvedahl',\n",
    " ',',\n",
    " 'james',\n",
    " 'h.',\n",
    " 'richardson',\n",
    " ',',\n",
    " '[',\n",
    " 'http',\n",
    " ':',\n",
    " '//portal.acm.org/citation.cfm',\n",
    " '?',\n",
    " 'id=800259.808982',\n",
    " 'maniac',\n",
    " ']',\n",
    " ',',\n",
    " 'proceedings',\n",
    " 'of',\n",
    " 'the',\n",
    " '1952',\n",
    " 'acm',\n",
    " 'national',\n",
    " 'meeting',\n",
    " '(',\n",
    " 'toronto',\n",
    " ')',\n",
    " ',',\n",
    " 'p.',\n",
    " '13-16',\n",
    " 'two',\n",
    " 'of',\n",
    " 'the',\n",
    " 'three',\n",
    " 'use',\n",
    " 'the',\n",
    " 'term',\n",
    " 'in',\n",
    " 'quotation',\n",
    " 'marks',\n",
    " '.',\n",
    " 'by',\n",
    " '1963',\n",
    " '``',\n",
    " 'debugging',\n",
    " \"''\",\n",
    " 'was',\n",
    " 'a',\n",
    " 'common',\n",
    " 'enough',\n",
    " 'term',\n",
    " 'to',\n",
    " 'be',\n",
    " 'mentioned',\n",
    " 'in',\n",
    " 'passing',\n",
    " 'without',\n",
    " 'explanation',\n",
    " 'on',\n",
    " 'page',\n",
    " '1',\n",
    " 'of',\n",
    " 'the',\n",
    " 'compatible',\n",
    " 'time-sharing',\n",
    " 'system|ctss',\n",
    " 'manual',\n",
    " '.',\n",
    " '[',\n",
    " 'http',\n",
    " ':',\n",
    " '//www.bitsavers.org/pdf/mit/ctss/ctss_programmersguide.pdf',\n",
    " 'the',\n",
    " 'compatible',\n",
    " 'time-sharing',\n",
    " 'system',\n",
    " ']',\n",
    " ',',\n",
    " 'm.i.t',\n",
    " '.',\n",
    " 'press',\n",
    " ',',\n",
    " '1963',\n",
    " 'kidwell',\n",
    " \"'s\",\n",
    " 'article',\n",
    " \"''stalking\",\n",
    " 'the',\n",
    " 'elusive',\n",
    " 'computer',\n",
    " 'bug',\n",
    " \"''\",\n",
    " 'peggy',\n",
    " 'aldrich',\n",
    " 'kidwell',\n",
    " ',',\n",
    " '[',\n",
    " 'http',\n",
    " ':',\n",
    " '//ieeexplore.ieee.org/xpl/freeabs_all.jsp',\n",
    " '?',\n",
    " 'tp=',\n",
    " '&',\n",
    " 'arnumber=728224',\n",
    " '&',\n",
    " 'isnumber=15706',\n",
    " 'stalking',\n",
    " 'the',\n",
    " 'elusive',\n",
    " 'computer',\n",
    " 'bug',\n",
    " ']',\n",
    " ',',\n",
    " 'ieee',\n",
    " 'annals',\n",
    " 'of',\n",
    " 'the',\n",
    " 'history',\n",
    " 'of',\n",
    " 'computing',\n",
    " ',',\n",
    " '1998.',\n",
    " 'discusses',\n",
    " 'the',\n",
    " 'etymology',\n",
    " 'of',\n",
    " '``',\n",
    " 'bug',\n",
    " \"''\",\n",
    " 'and',\n",
    " '``',\n",
    " 'debug',\n",
    " \"''\",\n",
    " 'in',\n",
    " 'greater',\n",
    " 'detail',\n",
    " '.',\n",
    " 'scope',\n",
    " 'as',\n",
    " 'software',\n",
    " 'and',\n",
    " 'electronic',\n",
    " 'systems',\n",
    " 'have',\n",
    " 'become',\n",
    " 'generally',\n",
    " 'more',\n",
    " 'complex',\n",
    " ',',\n",
    " 'the',\n",
    " 'various',\n",
    " 'common',\n",
    " 'debugging',\n",
    " 'techniques',\n",
    " 'have',\n",
    " 'expanded',\n",
    " 'with',\n",
    " 'more',\n",
    " 'methods',\n",
    " 'to',\n",
    " 'detect',\n",
    " 'anomalies',\n",
    " ',',\n",
    " 'assess',\n",
    " 'impact',\n",
    " ',',\n",
    " 'and',\n",
    " 'schedule',\n",
    " 'software',\n",
    " 'patches',\n",
    " 'or',\n",
    " 'full',\n",
    " 'updates',\n",
    " 'to',\n",
    " 'a',\n",
    " 'system',\n",
    " '.',\n",
    " 'the',\n",
    " 'words',\n",
    " '``',\n",
    " 'anomaly',\n",
    " \"''\",\n",
    " 'and',\n",
    " '``',\n",
    " 'discrepancy',\n",
    " \"''\",\n",
    " 'can',\n",
    " 'be',\n",
    " 'used',\n",
    " ',',\n",
    " 'as',\n",
    " 'being',\n",
    " 'more',\n",
    " 'neutral',\n",
    " 'terms',\n",
    " ',',\n",
    " 'to',\n",
    " 'avoid',\n",
    " 'the',\n",
    " 'words',\n",
    " '``',\n",
    " 'error',\n",
    " \"''\",\n",
    " 'and',\n",
    " '``',\n",
    " 'defect',\n",
    " \"''\",\n",
    " 'or',\n",
    " '``',\n",
    " 'bug',\n",
    " \"''\",\n",
    " 'where',\n",
    " 'there',\n",
    " 'might',\n",
    " 'be',\n",
    " 'an',\n",
    " 'implication',\n",
    " 'that',\n",
    " 'all',\n",
    " 'so-called',\n",
    " \"''errors\",\n",
    " \"''\",\n",
    " ',',\n",
    " \"''defects\",\n",
    " \"''\",\n",
    " 'or',\n",
    " \"''bugs\",\n",
    " \"''\",\n",
    " 'must',\n",
    " 'be',\n",
    " 'fixed',\n",
    " '(',\n",
    " 'at',\n",
    " 'all',\n",
    " 'costs',\n",
    " ')',\n",
    " '.',\n",
    " 'instead',\n",
    " ',',\n",
    " 'an',\n",
    " 'impact',\n",
    " 'assessment',\n",
    " 'can',\n",
    " 'be',\n",
    " 'made',\n",
    " 'to',\n",
    " 'determine',\n",
    " 'if',\n",
    " 'changes',\n",
    " 'to',\n",
    " 'remove',\n",
    " 'an',\n",
    " \"''anomaly\",\n",
    " \"''\",\n",
    " '(',\n",
    " 'or',\n",
    " \"''discrepancy\",\n",
    " \"''\",\n",
    " ')',\n",
    " 'would',\n",
    " 'be',\n",
    " 'cost-effective',\n",
    " 'for',\n",
    " 'the',\n",
    " 'system',\n",
    " ',',\n",
    " 'or',\n",
    " 'perhaps',\n",
    " 'a',\n",
    " 'scheduled',\n",
    " 'new',\n",
    " 'release',\n",
    " 'might',\n",
    " 'render',\n",
    " 'the',\n",
    " 'change',\n",
    " '(',\n",
    " 's',\n",
    " ')',\n",
    " 'unnecessary',\n",
    " '.',\n",
    " 'not',\n",
    " 'all',\n",
    " 'issues',\n",
    " 'are',\n",
    " 'life-critical',\n",
    " 'or',\n",
    " 'mission-critical',\n",
    " 'in',\n",
    " 'a',\n",
    " 'system',\n",
    " '.',\n",
    " 'also',\n",
    " ',',\n",
    " 'it',\n",
    " 'is',\n",
    " 'important',\n",
    " 'to',\n",
    " 'avoid',\n",
    " 'the',\n",
    " 'situation',\n",
    " 'where',\n",
    " 'a',\n",
    " 'change',\n",
    " 'might',\n",
    " 'be',\n",
    " 'more',\n",
    " 'upsetting',\n",
    " 'to',\n",
    " 'users',\n",
    " ',',\n",
    " 'long-term',\n",
    " ',',\n",
    " 'than',\n",
    " 'living',\n",
    " 'with',\n",
    " 'the',\n",
    " 'known',\n",
    " 'problem',\n",
    " '(',\n",
    " 's',\n",
    " ')',\n",
    " '(',\n",
    " 'where',\n",
    " 'the',\n",
    " '``',\n",
    " 'cure',\n",
    " 'would',\n",
    " 'be',\n",
    " 'worse',\n",
    " 'than',\n",
    " 'the',\n",
    " 'disease',\n",
    " \"''\",\n",
    " ')',\n",
    " '.',\n",
    " 'basing',\n",
    " 'decisions',\n",
    " 'of',\n",
    " 'the',\n",
    " 'acceptability',\n",
    " 'of',\n",
    " 'some',\n",
    " 'anomalies',\n",
    " 'can',\n",
    " 'avoid',\n",
    " 'a',\n",
    " 'culture',\n",
    " 'of',\n",
    " 'a',\n",
    " '``',\n",
    " 'zero-defects',\n",
    " \"''\",\n",
    " 'mandate',\n",
    " ',',\n",
    " 'where',\n",
    " 'people',\n",
    " 'might',\n",
    " 'be',\n",
    " 'tempted',\n",
    " 'to',\n",
    " 'deny',\n",
    " 'the',\n",
    " 'existence',\n",
    " 'of',\n",
    " 'problems',\n",
    " 'so',\n",
    " 'that',\n",
    " 'the',\n",
    " 'result',\n",
    " 'would',\n",
    " 'appear',\n",
    " 'as',\n",
    " 'zero',\n",
    " \"''defects\",\n",
    " \"''\",\n",
    " '.',\n",
    " 'considering',\n",
    " 'the',\n",
    " 'collateral',\n",
    " 'issues',\n",
    " ',',\n",
    " 'such',\n",
    " 'as',\n",
    " 'the',\n",
    " 'cost-versus-benefit',\n",
    " 'impact',\n",
    " 'assessment',\n",
    " ',',\n",
    " 'then',\n",
    " 'broader',\n",
    " 'debugging',\n",
    " 'techniques',\n",
    " 'will',\n",
    " 'expand',\n",
    " 'to',\n",
    " 'determine',\n",
    " 'the',\n",
    " 'frequency',\n",
    " 'of',\n",
    " 'anomalies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dmitrymikhaylov/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dmitrymikhaylov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 13), ('term', 12), ('computer', 10), ('system', 9), ('bug', 8), ('http', 8), ('used', 5), ('software', 4), ('change', 4), ('hopper', 4)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "english_stops = set(stopwords.words('english'))\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian_stops = set(stopwords.words('russian'))\n",
    "len(russian_stops)\n",
    "len(english_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and querying a corpus with `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
