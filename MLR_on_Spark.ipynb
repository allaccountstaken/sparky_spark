{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Homes').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Homes').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dmitrymikhaylov/Documents/code/spark/sparky_spark'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 10) #display a maximumn of 10 columns\n",
    "pd.set_option('display.max_rows', 10) #display a maximumn of 10 rows\n",
    "url = \"https://raw.githubusercontent.com/jkropko/DS-6001/master/localdata/anes_example.csv\"\n",
    "anes = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "caseid               float64\n",
       "turnout12              int64\n",
       "turnout12b           float64\n",
       "vote12               float64\n",
       "percent16              int64\n",
       "meet                   int64\n",
       "givefut                int64\n",
       "info                   int64\n",
       "march                  int64\n",
       "sign                   int64\n",
       "give12mo               int64\n",
       "compromise             int64\n",
       "ftobama              float64\n",
       "ftblack              float64\n",
       "ftwhite                int64\n",
       "fthisp               float64\n",
       "ftgay                float64\n",
       "ftjeb                float64\n",
       "fttrump              float64\n",
       "ftcarson             float64\n",
       "fthrc                float64\n",
       "ftrubio              float64\n",
       "ftcruz               float64\n",
       "ftsanders            float64\n",
       "ftfiorina            float64\n",
       "ftpolice             float64\n",
       "ftfem                float64\n",
       "fttrans              float64\n",
       "ftmuslim             float64\n",
       "ftsci                float64\n",
       "reg                    int64\n",
       "demcand                int64\n",
       "repcand              float64\n",
       "vote16jb             float64\n",
       "vote16bc             float64\n",
       "vote16tc             float64\n",
       "vote16mr             float64\n",
       "vote16dt             float64\n",
       "presjob                int64\n",
       "lazyb                  int64\n",
       "lazyw                  int64\n",
       "lazyh                  int64\n",
       "lazym                  int64\n",
       "violentb               int64\n",
       "violentw               int64\n",
       "violenth               int64\n",
       "violentm               int64\n",
       "econnow                int64\n",
       "econ12mo               int64\n",
       "pid1d                float64\n",
       "pid2d                 object\n",
       "pid1r                float64\n",
       "pid2r                 object\n",
       "pidstr               float64\n",
       "pidlean              float64\n",
       "lcself                 int64\n",
       "lcd                    int64\n",
       "lcr                    int64\n",
       "lchc                   int64\n",
       "lcbo                   int64\n",
       "lcdt                   int64\n",
       "lcmr                   int64\n",
       "lctc                   int64\n",
       "srv_spend              int64\n",
       "campfin                int64\n",
       "immig_legal            int64\n",
       "immig_numb             int64\n",
       "equalpay               int64\n",
       "parleave               int64\n",
       "crimespend             int64\n",
       "death                  int64\n",
       "terror_worry         float64\n",
       "terror_12mo          float64\n",
       "terror_local           int64\n",
       "relig_bc               int64\n",
       "relig_bcstr          float64\n",
       "relig_srv              int64\n",
       "relig_srvstr         float64\n",
       "incgap20               int64\n",
       "isis_troops            int64\n",
       "syrians_a            float64\n",
       "syrians_b            float64\n",
       "pc_a                 float64\n",
       "pc_b                 float64\n",
       "minwage                int64\n",
       "healthspend            int64\n",
       "childcare              int64\n",
       "getahead               int64\n",
       "ladder                 int64\n",
       "finwell                int64\n",
       "warm                   int64\n",
       "warmbad                int64\n",
       "warmcause              int64\n",
       "warmdo                 int64\n",
       "freetrade              int64\n",
       "stopwhite              int64\n",
       "stopblack              int64\n",
       "forcewhite             int64\n",
       "forceblack             int64\n",
       "stop_12mo            float64\n",
       "arrested_12mo        float64\n",
       "charged_12mo         float64\n",
       "jailed_12mo          float64\n",
       "convict_12mo         float64\n",
       "famstop_12mo         float64\n",
       "stop_ever            float64\n",
       "arrested_ever        float64\n",
       "charged_ever         float64\n",
       "jailed_ever          float64\n",
       "convict_ever         float64\n",
       "famstop_ever         float64\n",
       "pk_deficit             int64\n",
       "pk_sen               float64\n",
       "pk_spend               int64\n",
       "birthright_a         float64\n",
       "birthright_b         float64\n",
       "femoff_jobs          float64\n",
       "femoff_ed            float64\n",
       "femoff_spend         float64\n",
       "femoff_issues        float64\n",
       "lpres_pleased          int64\n",
       "lpres_immig          float64\n",
       "lpres_la             float64\n",
       "vaccine                int64\n",
       "autism                 int64\n",
       "bo_muslim              int64\n",
       "bo_confid            float64\n",
       "amer_ident             int64\n",
       "race_ident             int64\n",
       "whitework            float64\n",
       "whitejob             float64\n",
       "wguilt1              float64\n",
       "wguilt2              float64\n",
       "wguilt3              float64\n",
       "buycott                int64\n",
       "boycott                int64\n",
       "skintone_mob           int64\n",
       "skintone             float64\n",
       "skin_discrim           int64\n",
       "africanam10_1        float64\n",
       "white10_1            float64\n",
       "hispanic10_1         float64\n",
       "asianam10_1          float64\n",
       "nativeam10_1           int64\n",
       "other10_1            float64\n",
       "other10_open          object\n",
       "birthyr                int64\n",
       "gender                 int64\n",
       "race                   int64\n",
       "race_other            object\n",
       "educ                   int64\n",
       "marstat                int64\n",
       "speakspanish           int64\n",
       "employ               float64\n",
       "employ_t              object\n",
       "faminc               float64\n",
       "faminc2                int64\n",
       "state                float64\n",
       "votereg                int64\n",
       "pid3                   int64\n",
       "pid7                 float64\n",
       "ideo5                  int64\n",
       "newsint                int64\n",
       "pew_bornagain          int64\n",
       "pew_churatd            int64\n",
       "religpew             float64\n",
       "religpew_t            object\n",
       "ever_vs_12mo_rand      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None) #to see all the variables\n",
    "anes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 168 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   caseid             float64\n",
      " 1   turnout12          int64  \n",
      " 2   turnout12b         float64\n",
      " 3   vote12             float64\n",
      " 4   percent16          int64  \n",
      " 5   meet               int64  \n",
      " 6   givefut            int64  \n",
      " 7   info               int64  \n",
      " 8   march              int64  \n",
      " 9   sign               int64  \n",
      " 10  give12mo           int64  \n",
      " 11  compromise         int64  \n",
      " 12  ftobama            float64\n",
      " 13  ftblack            float64\n",
      " 14  ftwhite            int64  \n",
      " 15  fthisp             float64\n",
      " 16  ftgay              float64\n",
      " 17  ftjeb              float64\n",
      " 18  fttrump            float64\n",
      " 19  ftcarson           float64\n",
      " 20  fthrc              float64\n",
      " 21  ftrubio            float64\n",
      " 22  ftcruz             float64\n",
      " 23  ftsanders          float64\n",
      " 24  ftfiorina          float64\n",
      " 25  ftpolice           float64\n",
      " 26  ftfem              float64\n",
      " 27  fttrans            float64\n",
      " 28  ftmuslim           float64\n",
      " 29  ftsci              float64\n",
      " 30  reg                int64  \n",
      " 31  demcand            int64  \n",
      " 32  repcand            float64\n",
      " 33  vote16jb           float64\n",
      " 34  vote16bc           float64\n",
      " 35  vote16tc           float64\n",
      " 36  vote16mr           float64\n",
      " 37  vote16dt           float64\n",
      " 38  presjob            int64  \n",
      " 39  lazyb              int64  \n",
      " 40  lazyw              int64  \n",
      " 41  lazyh              int64  \n",
      " 42  lazym              int64  \n",
      " 43  violentb           int64  \n",
      " 44  violentw           int64  \n",
      " 45  violenth           int64  \n",
      " 46  violentm           int64  \n",
      " 47  econnow            int64  \n",
      " 48  econ12mo           int64  \n",
      " 49  pid1d              float64\n",
      " 50  pid2d              object \n",
      " 51  pid1r              float64\n",
      " 52  pid2r              object \n",
      " 53  pidstr             float64\n",
      " 54  pidlean            float64\n",
      " 55  lcself             int64  \n",
      " 56  lcd                int64  \n",
      " 57  lcr                int64  \n",
      " 58  lchc               int64  \n",
      " 59  lcbo               int64  \n",
      " 60  lcdt               int64  \n",
      " 61  lcmr               int64  \n",
      " 62  lctc               int64  \n",
      " 63  srv_spend          int64  \n",
      " 64  campfin            int64  \n",
      " 65  immig_legal        int64  \n",
      " 66  immig_numb         int64  \n",
      " 67  equalpay           int64  \n",
      " 68  parleave           int64  \n",
      " 69  crimespend         int64  \n",
      " 70  death              int64  \n",
      " 71  terror_worry       float64\n",
      " 72  terror_12mo        float64\n",
      " 73  terror_local       int64  \n",
      " 74  relig_bc           int64  \n",
      " 75  relig_bcstr        float64\n",
      " 76  relig_srv          int64  \n",
      " 77  relig_srvstr       float64\n",
      " 78  incgap20           int64  \n",
      " 79  isis_troops        int64  \n",
      " 80  syrians_a          float64\n",
      " 81  syrians_b          float64\n",
      " 82  pc_a               float64\n",
      " 83  pc_b               float64\n",
      " 84  minwage            int64  \n",
      " 85  healthspend        int64  \n",
      " 86  childcare          int64  \n",
      " 87  getahead           int64  \n",
      " 88  ladder             int64  \n",
      " 89  finwell            int64  \n",
      " 90  warm               int64  \n",
      " 91  warmbad            int64  \n",
      " 92  warmcause          int64  \n",
      " 93  warmdo             int64  \n",
      " 94  freetrade          int64  \n",
      " 95  stopwhite          int64  \n",
      " 96  stopblack          int64  \n",
      " 97  forcewhite         int64  \n",
      " 98  forceblack         int64  \n",
      " 99  stop_12mo          float64\n",
      " 100 arrested_12mo      float64\n",
      " 101 charged_12mo       float64\n",
      " 102 jailed_12mo        float64\n",
      " 103 convict_12mo       float64\n",
      " 104 famstop_12mo       float64\n",
      " 105 stop_ever          float64\n",
      " 106 arrested_ever      float64\n",
      " 107 charged_ever       float64\n",
      " 108 jailed_ever        float64\n",
      " 109 convict_ever       float64\n",
      " 110 famstop_ever       float64\n",
      " 111 pk_deficit         int64  \n",
      " 112 pk_sen             float64\n",
      " 113 pk_spend           int64  \n",
      " 114 birthright_a       float64\n",
      " 115 birthright_b       float64\n",
      " 116 femoff_jobs        float64\n",
      " 117 femoff_ed          float64\n",
      " 118 femoff_spend       float64\n",
      " 119 femoff_issues      float64\n",
      " 120 lpres_pleased      int64  \n",
      " 121 lpres_immig        float64\n",
      " 122 lpres_la           float64\n",
      " 123 vaccine            int64  \n",
      " 124 autism             int64  \n",
      " 125 bo_muslim          int64  \n",
      " 126 bo_confid          float64\n",
      " 127 amer_ident         int64  \n",
      " 128 race_ident         int64  \n",
      " 129 whitework          float64\n",
      " 130 whitejob           float64\n",
      " 131 wguilt1            float64\n",
      " 132 wguilt2            float64\n",
      " 133 wguilt3            float64\n",
      " 134 buycott            int64  \n",
      " 135 boycott            int64  \n",
      " 136 skintone_mob       int64  \n",
      " 137 skintone           float64\n",
      " 138 skin_discrim       int64  \n",
      " 139 africanam10_1      float64\n",
      " 140 white10_1          float64\n",
      " 141 hispanic10_1       float64\n",
      " 142 asianam10_1        float64\n",
      " 143 nativeam10_1       int64  \n",
      " 144 other10_1          float64\n",
      " 145 other10_open       object \n",
      " 146 birthyr            int64  \n",
      " 147 gender             int64  \n",
      " 148 race               int64  \n",
      " 149 race_other         object \n",
      " 150 educ               int64  \n",
      " 151 marstat            int64  \n",
      " 152 speakspanish       int64  \n",
      " 153 employ             float64\n",
      " 154 employ_t           object \n",
      " 155 faminc             float64\n",
      " 156 faminc2            int64  \n",
      " 157 state              float64\n",
      " 158 votereg            int64  \n",
      " 159 pid3               int64  \n",
      " 160 pid7               float64\n",
      " 161 ideo5              int64  \n",
      " 162 newsint            int64  \n",
      " 163 pew_bornagain      int64  \n",
      " 164 pew_churatd        int64  \n",
      " 165 religpew           float64\n",
      " 166 religpew_t         object \n",
      " 167 ever_vs_12mo_rand  int64  \n",
      "dtypes: float64(76), int64(86), object(6)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "anes.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header', 'true').csv('house_data.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- sqft_living: integer (nullable = true)\n",
      " |-- sqft_lot: integer (nullable = true)\n",
      " |-- floors: double (nullable = true)\n",
      " |-- waterfront: integer (nullable = true)\n",
      " |-- view: integer (nullable = true)\n",
      " |-- condition: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- sqft_above: integer (nullable = true)\n",
      " |-- sqft_basement: integer (nullable = true)\n",
      " |-- yr_built: integer (nullable = true)\n",
      " |-- yr_renovated: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- sqft_living15: integer (nullable = true)\n",
      " |-- sqft_lot15: integer (nullable = true)\n",
      " |-- num: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+---+\n",
      "|        id|           date|   price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|condition|grade|sqft_above|sqft_basement|yr_built|yr_renovated|zipcode|    lat|    long|sqft_living15|sqft_lot15|num|\n",
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+---+\n",
      "|7129300520|20141013T000000|221900.0|       3|      1.0|       1180|    5650|   1.0|         0|   0|        3|    7|      1180|            0|    1955|           0|  98178|47.5112|-122.257|         1340|      5650|  1|\n",
      "|6414100192|20141209T000000|538000.0|       3|     2.25|       2570|    7242|   2.0|         0|   0|        3|    7|      2170|          400|    1951|        1991|  98125| 47.721|-122.319|         1690|      7639|  1|\n",
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|bedrooms|bedrooms|\n",
      "+--------+--------+\n",
      "|       3|       3|\n",
      "|       3|       3|\n",
      "+--------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['bedrooms','bedrooms']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|             price|         bedrooms|         bathrooms|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|             21613|            21613|             21613|\n",
      "|   mean| 540182.1587933188| 3.37084162309721|2.1147573219821405|\n",
      "| stddev|367362.23171800876|0.930061831147451| 0.770163157217741|\n",
      "|    min|           75000.0|                0|               0.0|\n",
      "|    max|         7700000.0|               33|               8.0|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe(['price', 'bedrooms', 'bathrooms']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import log\n",
    "\n",
    "df_pyspark = df_pyspark.withColumn(\"logvalue\", log(\"price\"))\n",
    "\n",
    "#df.withColumn(\"bla\", log(\"double_col\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|   price|          logvalue|\n",
      "+--------+------------------+\n",
      "|221900.0|12.309982108920686|\n",
      "|538000.0|13.195613839143922|\n",
      "+--------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['price', 'logvalue']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[price: double, bedrooms: int, bathrooms: double, sqft_living: int, sqft_lot: int, floors: double, waterfront: int, view: int, condition: int, grade: int, sqft_above: int, sqft_basement: int, yr_built: int, yr_renovated: int, zipcode: int, sqft_living15: int, sqft_lot15: int, num: int, logvalue: double]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.drop('date', 'id', 'lat', 'long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = df_pyspark.filter('bedrooms > 11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+---+------------------+\n",
      "|        id|           date|   price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|condition|grade|sqft_above|sqft_basement|yr_built|yr_renovated|zipcode|    lat|    long|sqft_living15|sqft_lot15|num|          logvalue|\n",
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+---+------------------+\n",
      "|2402100895|20140625T000000|640000.0|      33|     1.75|       1620|    6000|   1.0|         0|   0|        5|    7|      1040|          580|    1947|           0|  98103|47.6878|-122.331|         1330|      4700|  1|13.369223455335854|\n",
      "+----------+---------------+--------+--------+---------+-----------+--------+------+----------+----+---------+-----+----------+-------------+--------+------------+-------+-------+--------+-------------+----------+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selection.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------+\n",
      "|bedrooms|max(price)|max(bedrooms)|\n",
      "+--------+----------+-------------+\n",
      "|       1| 1250000.0|            1|\n",
      "|       6| 7700000.0|            6|\n",
      "|       3| 3800000.0|            3|\n",
      "|       5| 7060000.0|            5|\n",
      "|       9| 1400000.0|            9|\n",
      "|       4| 4490000.0|            4|\n",
      "|       8| 3300000.0|            8|\n",
      "|       7| 3200000.0|            7|\n",
      "|      10| 1150000.0|           10|\n",
      "|      11|  520000.0|           11|\n",
      "|      33|  640000.0|           33|\n",
      "|       2| 3280000.0|            2|\n",
      "|       0| 1300000.0|            0|\n",
      "+--------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['price', 'bedrooms']).groupBy('bedrooms').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLR_in_R.pdf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'date',\n",
       " 'price',\n",
       " 'bedrooms',\n",
       " 'bathrooms',\n",
       " 'sqft_living',\n",
       " 'sqft_lot',\n",
       " 'floors',\n",
       " 'waterfront',\n",
       " 'view',\n",
       " 'condition',\n",
       " 'grade',\n",
       " 'sqft_above',\n",
       " 'sqft_basement',\n",
       " 'yr_built',\n",
       " 'yr_renovated',\n",
       " 'zipcode',\n",
       " 'lat',\n",
       " 'long',\n",
       " 'sqft_living15',\n",
       " 'sqft_lot15',\n",
       " 'num',\n",
       " 'logvalue']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureAssembler = VectorAssembler(inputCols=['bedrooms', 'bathrooms', 'floors'], outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureAssembler.transform(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Independent Features|\n",
      "+--------------------+\n",
      "|       [3.0,1.0,1.0]|\n",
      "|      [3.0,2.25,2.0]|\n",
      "|       [2.0,1.0,1.0]|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select(\"Independent Features\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|          logvalue|Independent Features|\n",
      "+------------------+--------------------+\n",
      "|12.309982108920686|       [3.0,1.0,1.0]|\n",
      "|13.195613839143922|      [3.0,2.25,2.0]|\n",
      "|12.100712129872347|       [2.0,1.0,1.0]|\n",
      "+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_dataset = output.select(['logvalue', 'Independent Features'])\n",
    "finalized_dataset.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression \n",
    "train_data, test_data = finalized_dataset.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f852c2e6310>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dmitrymikhaylov/opt/anaconda3/lib/python3.8/site-packages/pyspark/ml/wrapper.py\", line 39, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'LinearRegression' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "regr = LinearRegression(featuresCol=\"Independent Features\", labelCol=\"logvalue\")\n",
    "regr = regr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0498, 0.3233, 0.0559])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.112741435876528"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+\n",
      "|          logvalue|Independent Features|        prediction|\n",
      "+------------------+--------------------+------------------+\n",
      "| 11.26446410567173|       [2.0,1.0,1.0]| 12.59150574718085|\n",
      "|11.289781913656018|      [1.0,0.75,1.0]|12.460902813068595|\n",
      "|11.326595886778735|       [2.0,1.0,1.0]| 12.59150574718085|\n",
      "|11.350406535472453|       [2.0,1.0,1.0]| 12.59150574718085|\n",
      "| 11.36789969291997|       [3.0,1.0,1.0]|12.641289364067557|\n",
      "|11.396391648714276|       [3.0,1.0,1.0]|12.641289364067557|\n",
      "|11.407564949312402|       [1.0,1.0,1.0]|12.541722130294144|\n",
      "|11.477298287327077|       [3.0,1.0,1.0]|12.641289364067557|\n",
      "|11.502875129116727|       [2.0,1.0,1.0]| 12.59150574718085|\n",
      "|11.512925464970229|       [2.0,1.0,1.0]| 12.59150574718085|\n",
      "+------------------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmitrymikhaylov/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pred_results = regr.evaluate(test_data)\n",
    "pred_results.predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34453702107444745, 0.18589940619284742)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError, pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.328485851990698"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- bedrooms: integer (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- sqft_living: integer (nullable = true)\n",
      " |-- sqft_lot: integer (nullable = true)\n",
      " |-- floors: double (nullable = true)\n",
      " |-- waterfront: integer (nullable = true)\n",
      " |-- view: integer (nullable = true)\n",
      " |-- condition: integer (nullable = true)\n",
      " |-- grade: integer (nullable = true)\n",
      " |-- sqft_above: integer (nullable = true)\n",
      " |-- sqft_basement: integer (nullable = true)\n",
      " |-- yr_built: integer (nullable = true)\n",
      " |-- yr_renovated: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- sqft_living15: integer (nullable = true)\n",
      " |-- sqft_lot15: integer (nullable = true)\n",
      " |-- num: integer (nullable = true)\n",
      " |-- logvalue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='waterfront', outputCol='waterfront_indexed')\n",
    "df_r = indexer.fit(df_pyspark).transform(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|waterfront|waterfront_indexed|\n",
      "+----------+------------------+\n",
      "|         0|               0.0|\n",
      "|         0|               0.0|\n",
      "|         0|               0.0|\n",
      "+----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_r.select(['waterfront', 'waterfront_indexed']).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['org_idx'],outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights)\n",
    "flights_onehot = onehot.transform(flights)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().\\\n",
    "sort('org_idx').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Evaluating SLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Categorical Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60 / regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# Average minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Buckets for Continuous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[0, 3, 6, 9, 12, 15, 18, 21, 24],inputCol='depart', outputCol='depart_bucket')\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(flights)\n",
    "bucketed.select('depart', 'depart_bucket').show(5)\n",
    "\n",
    "# Create a one-hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['depart_bucket'],outputCols=['depart_dummy'])\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
    "flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').\\\n",
    "show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fit linear regression model to training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Make predictions on testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "#rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical strings to index values\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# One-hot encode index values\n",
    "onehot = OneHotEncoderEstimator(\n",
    "    inputCols=['org_idx', 'dow'],\n",
    "    outputCols=['org_dummy', 'dow_dummy']\n",
    ")\n",
    "\n",
    "# Assemble predictors into a single column\n",
    "assembler = VectorAssembler(inputCols=['org_dummy', 'dow_dummy', 'km'], outputCol='features')\n",
    "\n",
    "# A linear regression object\n",
    "regression = LinearRegression(labelCol='duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty parameter grid\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# Create objects for building and evaluating a regression model\n",
    "regression = LinearRegression(labelCol='duration')\n",
    "evaluator = RegressionEvaluator(labelCol='duration')\n",
    "\n",
    "# Create a cross validator\n",
    "cv = CrossValidator(estimator=regression, \n",
    "estimatorParamMaps=params, \n",
    "evaluator=evaluator, \n",
    "numFolds=5)\n",
    "\n",
    "# Train and test model on multiple folds of the training data\n",
    "cv = cv.fit(flights_train)\n",
    "\n",
    "# NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter grid\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# Add grids for two parameters\n",
    "params = params.addGrid(regression.regParam, \n",
    "[0.01, 0.1, 1, 10])\\\n",
    ".addGrid(regression.elasticNetParam, [0, 0.5, 1])\n",
    "\n",
    "# Build the parameter grid\n",
    "params = params.build()\n",
    "print('Number of models to be tested: ', len(params))\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "estimatorParamMaps=params, \n",
    "evaluator=evaluator, \n",
    "numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from cross validation\n",
    "best_model = cv.bestModel\n",
    "\n",
    "# Look at the stages in the best model\n",
    "print(best_model.stages)\n",
    "\n",
    "# Get the parameters for the LinearRegression object in the best model\n",
    "best_model.stages[3].extractParamMap()\n",
    "\n",
    "# Generate predictions on testing data using the best model then calculate RMSE\n",
    "predictions = best_model.transform(flights_test)\n",
    "evaluator.evaluate(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
