{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Final plan:\
1. AutoML regression produces complex but accurate model\
2. Manual regression, assumptions, outliers, quintile predictions\
3. Comparing feature importances from manual and AutoML models\
4. AutoML classification\
5. Manual classification, optimizing for FP, i.e potentially 1M homes that are priced below\
6. Bringing it all together - generate x350 synthetic data on AWS EC2 and use Spark to run regression and classification, possibly XGboost \
\
\'97\'97\
\
\
\
General intro into regressions:\
https://www.geeksforgeeks.org/advantages-and-disadvantages-of-different-regression-models/\
Linear, Polynomial, Support Vector, Decision Tree, Random Forest (what about KNN and NN?)\
\
Limitations of Random Forest:\
https://towardsdatascience.com/a-limitation-of-random-forest-regression-db8ed7419e9f\
\
Covariate Shift:\
https://www.analyticsvidhya.com/blog/2017/07/covariate-shift-the-hidden-problem-of-real-world-data-science/\
\
NB: Target variable or Prior Probability Shift example:\
 interest rates lower - financing abundant - home prices much higher\
\
Context:\
\
Half above and half below:\
https://www.zdnet.com/article/zillow-machine-learning-and-data-in-real-estate/\
\
Others:\
https://www.bloomberg.com/news/articles/2021-11-01/zillow-selling-7-000-homes-for-2-8-billion-after-flipping-halt\
https://analyticsindiamag.com/zillows-great-data-science-disaster/\
https://towardsdatascience.com/invaluable-data-science-lessons-to-learn-from-the-failure-of-zillows-flipping-business-25fdc218a62\
\
Lecture Plan:\
1. Upload raw data to Azure AutoML for regression task\
2. Manual ETA in R, SLR model, assumptions check, MLR model regsubsets baseline, \
2.2 Assumptions checks, outliers, measures of influence \
3. Split into several files with train/test routine\
4. Create AWS cluster, upload 2 files with same schema, notebook with spark df\
5. Grid search with sklearn or spark ML, create global feature importances chart\
6. Back to Azure AutoML, compare model performance and feature importances\
7. TBD \'97>  compare models on oos for price prediction: 3 homes with 10 bedrooms, 1 home with 11 bedrooms, and 1 with 33. How the rules of neighborhood can hurt your prediction.\
8. Consider quantile regression in Spark}